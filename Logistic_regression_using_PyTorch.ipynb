{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Logistic regression using PyTorch.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMtmDKrawnj5YVfA6pyc2lV",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/myDSMLProjects/PyTorch-Fundamentals/blob/master/Logistic_regression_using_PyTorch.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9HQXPNuoDh4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "import torch.optim as optim\n",
        "import numpy as np"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Izqlx5ddpgVi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Hyperparamters\n",
        "input_size = 28*28\n",
        "num_classes = 10\n",
        "num_epochs = 5\n",
        "batch_size = 100\n",
        "learning_rate = 0.001"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVthVv7Cp9_C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MNIST dataset with images and labels\n",
        "train_dataset = datasets.MNIST(train=True, download=True, root='data/', transform=transforms.ToTensor())\n",
        "test_dataset = datasets.MNIST(train=False, download=True, root='data/', transform=transforms.ToTensor())\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8t9UGVUVrUv0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Logistic Regression model\n",
        "model = nn.Linear(in_features = input_size, out_features=num_classes)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(params = model.parameters(), lr=learning_rate)"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s02lxcYBsDRZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 535
        },
        "outputId": "7c610756-7f6b-4e87-cdb4-0cbe944716f6"
      },
      "source": [
        "# Train the model\n",
        "loss_data = []\n",
        "total_steps = len(train_loader)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  for batch_idx, (images, labels) in enumerate(train_loader):\n",
        "    predicted_labels = model(images.reshape(-1, input_size))\n",
        "    loss = criterion(predicted_labels, labels)\n",
        "    loss_data.append(loss.item())\n",
        "    # Go Backwads and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (batch_idx+1)%100==0:\n",
        "      print(f'Epoch {epoch+1}/{num_epochs} step {batch_idx+1}/{total_steps} Loss:{loss.item()}')\n",
        "\n",
        "\n",
        "# Test the model\n",
        "# In test phase we dont need to compute the grads to be memmory efficient\n",
        "with torch.no_grad():\n",
        "  correct=0\n",
        "  total=0\n",
        "\n",
        "  for images, labels in test_loader:\n",
        "    predicted_labels = model(images.reshape(-1, input_size))\n",
        "    _, predicted = predicted_labels.max(1)\n",
        "    total+=labels.size(0)\n",
        "    correct+=(predicted==labels).sum()\n",
        "\n",
        "  print('Accuracy of the model on the 10000 test images: {} %'.format((correct//total)*100))\n",
        "\n",
        "# Save the model checkpoint\n"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/5 step 100/600 Loss:0.7736162543296814\n",
            "Epoch 1/5 step 200/600 Loss:0.561775803565979\n",
            "Epoch 1/5 step 300/600 Loss:0.44546499848365784\n",
            "Epoch 1/5 step 400/600 Loss:0.5257524847984314\n",
            "Epoch 1/5 step 500/600 Loss:0.4159564971923828\n",
            "Epoch 1/5 step 600/600 Loss:0.3748333752155304\n",
            "Epoch 2/5 step 100/600 Loss:0.4140099585056305\n",
            "Epoch 2/5 step 200/600 Loss:0.3236123323440552\n",
            "Epoch 2/5 step 300/600 Loss:0.35383445024490356\n",
            "Epoch 2/5 step 400/600 Loss:0.3255329430103302\n",
            "Epoch 2/5 step 500/600 Loss:0.30274373292922974\n",
            "Epoch 2/5 step 600/600 Loss:0.37548118829727173\n",
            "Epoch 3/5 step 100/600 Loss:0.30213025212287903\n",
            "Epoch 3/5 step 200/600 Loss:0.23802918195724487\n",
            "Epoch 3/5 step 300/600 Loss:0.3525805175304413\n",
            "Epoch 3/5 step 400/600 Loss:0.22651401162147522\n",
            "Epoch 3/5 step 500/600 Loss:0.3654022216796875\n",
            "Epoch 3/5 step 600/600 Loss:0.2681204378604889\n",
            "Epoch 4/5 step 100/600 Loss:0.28025034070014954\n",
            "Epoch 4/5 step 200/600 Loss:0.23157086968421936\n",
            "Epoch 4/5 step 300/600 Loss:0.3531401455402374\n",
            "Epoch 4/5 step 400/600 Loss:0.3875012695789337\n",
            "Epoch 4/5 step 500/600 Loss:0.3696579039096832\n",
            "Epoch 4/5 step 600/600 Loss:0.1867026686668396\n",
            "Epoch 5/5 step 100/600 Loss:0.23974250257015228\n",
            "Epoch 5/5 step 200/600 Loss:0.14720571041107178\n",
            "Epoch 5/5 step 300/600 Loss:0.2881152629852295\n",
            "Epoch 5/5 step 400/600 Loss:0.2185305804014206\n",
            "Epoch 5/5 step 500/600 Loss:0.2626831531524658\n",
            "Epoch 5/5 step 600/600 Loss:0.30865678191185\n",
            "Accuracy of the model on the 10000 test images: 0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5EKKM8Apa1QW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.save(model.state_dict(), 'model.ckpt')\n"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jailq3BYbu5R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 15,
      "outputs": []
    }
  ]
}